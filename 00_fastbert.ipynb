{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp fastbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastBert\n",
    "\n",
    "> fast.ai API customization for separateable sequence Bert model\n",
    "\n",
    "<img src=\"data/fastbert.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# pytorch\n",
    "import torch\n",
    "\n",
    "# transformers\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig\n",
    "\n",
    "# fast.ai\n",
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the other models from huggingfaces go to\n",
    "# https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta\n",
    "# the notebook will work as well\n",
    "# just pay attention at the customized model section\n",
    "\n",
    "model_class, tokenizer_class, config_class = BertForSequenceClassification, BertTokenizer, BertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert + fast.ai\n",
    "\n",
    "Customization is following the work of https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta. I would recommemend to read the pipeline before, if not done already. When additional work is done we are explaining the steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going into the implementations from former collueges and customize them to make the tokenizer capable for our special needs. \n",
    "\n",
    "Namely we have to tokenize a List[sequence] element, where sequence in string form, to return a List[tokens] respecting the seperated format. \n",
    "\n",
    "The respecting format is:\n",
    "\n",
    "\\begin{equation*}\n",
    "[CLS] + tokens(seq_1) + tokens(seq_2) + [SEP] + tokens(seq_3) + [SEP]\n",
    "\\end{equation*}\n",
    "\n",
    "what will be cut by the max_len parameter of Bert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TransformersBaseTokenizer(BaseTokenizer):\n",
    "    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n",
    "    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', max_len=64, **kwargs):\n",
    "        self._pretrained_tokenizer = pretrained_tokenizer\n",
    "        self.max_seq_len = max_len\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def __call__(self, *args, **kwargs): \n",
    "        return self\n",
    "\n",
    "    def tokenizer(self, t:List[str]) -> List[str]:\n",
    "        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n",
    "        CLS = self._pretrained_tokenizer.cls_token\n",
    "        SEP = self._pretrained_tokenizer.sep_token\n",
    "        turns = [self._pretrained_tokenizer.tokenize(turn) for turn in t]\n",
    "        tokens = [CLS] + turns[0] + turns[1] + [SEP] + turns[2] + [SEP]\n",
    "        return tokens[:self.max_seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Tokenizer class we just change the type annotations from str to List[str]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SeqTokenizer(Tokenizer):\n",
    "    \"Put together rules and a tokenizer function to tokenize text with multiprocessing.\"\n",
    "    def __init__(self, tok_func:Callable=SpacyTokenizer, lang:str='en', pre_rules:ListRules=None,\n",
    "                 post_rules:ListRules=None, special_cases:Collection[str]=None, n_cpus:int=None):\n",
    "        self.tok_func,self.lang,self.special_cases = tok_func,lang,special_cases\n",
    "        self.pre_rules  = ifnone(pre_rules,  defaults.text_pre_rules )\n",
    "        self.post_rules = ifnone(post_rules, defaults.text_post_rules)\n",
    "        self.special_cases = special_cases if special_cases is not None else defaults.text_spec_tok\n",
    "        self.n_cpus = ifnone(n_cpus, defaults.cpus)\n",
    "\n",
    "    def process_text(self, t:List[str], tok:BaseTokenizer) -> List[str]:\n",
    "        \"Process one text `t` with tokenizer `tok`.\"\n",
    "        for rule in self.pre_rules: t = rule(t)\n",
    "        toks = tok.tokenizer(t)\n",
    "        for rule in self.post_rules: toks = rule(toks)\n",
    "        return toks\n",
    "\n",
    "    def _process_all_1(self, texts:Collection[List[str]]) -> List[List[str]]:\n",
    "        \"Process a list of `texts` in one process.\"\n",
    "        tok = self.tok_func(self.lang)\n",
    "        if self.special_cases: tok.add_special_cases(self.special_cases)\n",
    "        return [self.process_text(t, tok) for t in texts]\n",
    "\n",
    "    def process_all(self, texts:Collection[List[str]]) -> List[List[str]]:\n",
    "        \"Process a list of `texts`.\"\n",
    "        if self.n_cpus <= 1: return self._process_all_1(texts)\n",
    "        with ProcessPoolExecutor(self.n_cpus) as e:\n",
    "            return sum(e.map(self._process_all_1, partition_by_cores(texts, self.n_cpus)), [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the challenge is to make the TokenizeProcessor class able to use the new input form of List[sequence]. \n",
    "\n",
    "In fast.ai the TokenizeProcessor class processes a list of str to a concatination and uses then the tokenizer for the whole text. So we built a customized version for the Tokenizer. Here we changed the class functions to use our customized tokenizer and got rid of the function for concatination (\\_join\\_texts). \n",
    "\n",
    "It would be more elegant to use the \\_join\\_texts function in a customized form to do our approach by concatinating the sentences via a special token. One problem with that approach would be that the BertTokenizer is not able to distinguish between text and special tokens in the text. We have to add the special tokens after tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "class SeqTokenizeProcessor(TokenizeProcessor):\n",
    "    \"`PreProcessor` that tokenizes the texts in `ds`.\"\n",
    "    def __init__(self, ds:ItemList=None, tokenizer:Tokenizer=None, chunksize:int=10000, \n",
    "                 mark_fields:bool=False, include_bos:bool=True, include_eos:bool=False):\n",
    "        self.tokenizer,self.chunksize,self.mark_fields = ifnone(tokenizer, Tokenizer()),chunksize,mark_fields\n",
    "        self.include_bos, self.include_eos = include_bos, include_eos\n",
    "\n",
    "    def process_one(self, item):\n",
    "        return self.tokenizer._process_all_1(item)[0]\n",
    "\n",
    "    def process(self, ds):\n",
    "        tokens = []\n",
    "        for i in progress_bar(range(0,len(ds),self.chunksize), leave=False):\n",
    "            tokens += self.tokenizer.process_all(ds.items[i:i+self.chunksize])\n",
    "        ds.items = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting it up \n",
    "# pretrained models can be shown by\n",
    "# model_class.pretrained_model_archive_map.keys()\n",
    "pretrained_model_name = 'bert-large-uncased-whole-word-masking'\n",
    "\n",
    "transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n",
    "transformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\n",
    "fastai_tokenizer = SeqTokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])\n",
    "tokenize_processor = SeqTokenizeProcessor(tokenizer=fastai_tokenizer, include_bos=False, include_eos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing of the TokenizeProcessor subclass\n",
    "test_item_1 = ['turn one','turn two','turn three.']\n",
    "test_item_2 = ['turn four', 'turn five', 'turn six.']\n",
    "test_tokens_1 = ['[CLS]', 'turn', 'one', 'turn', 'two', '[SEP]', \n",
    "               'turn', 'three', '.', '[SEP]']\n",
    "test_tokens_2 = ['[CLS]', 'turn', 'four', 'turn', 'five', '[SEP]', \n",
    "               'turn', 'six', '.', '[SEP]']\n",
    "\n",
    "test_items = ItemList(items = [test_item_1, test_item_2])\n",
    "\n",
    "try:\n",
    "    tokenize_processor.process(test_items)\n",
    "except e:\n",
    "    print(e) \n",
    "    \n",
    "assert tokenize_processor.process_one([test_item_1]) == test_tokens_1 \n",
    "assert test_items[0] == test_tokens_1\n",
    "assert test_items[1] == test_tokens_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the tokenizer works like we wanted it to be, we have to move to the next problem. We need not only the input_ids of the sentences but also the attention_mask and token_type_ids. \n",
    "\n",
    "When looking into the model setup we can do a simple trick by using utility functions in the forward pass. Hence we have two utitliy functions retrieving the masks from an input_ids batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def segment(input_ids):\n",
    "    \"\"\"util function for token_type_ids in bert\"\"\"\n",
    "    segment_ids = input_ids.clone().cpu() # make sure VRAM will not explode\n",
    "    segs = (segment_ids==102).nonzero().cpu().numpy()\n",
    "    state = -1\n",
    "    # current tensor\n",
    "    cur = -1\n",
    "    for seg in segs:\n",
    "        if cur != seg[0]:\n",
    "            cur = seg[0]\n",
    "            segment_ids[cur][:seg[1]] = 0\n",
    "            segment_ids[cur] = \\\n",
    "            (segment_ids[cur]!=0).type(segment_ids[cur].type())\n",
    "\n",
    "    segs_set = set(segs.transpose()[0])\n",
    "    if segs_set != segment_ids.shape[0]:\n",
    "        for i in (set(range(segment_ids.shape[0])) - segs_set):\n",
    "            segment_ids[i] = 0\n",
    "\n",
    "    return segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the segment function\n",
    "# 101 - CLS token, 102 - SEP token\n",
    "# example batch is in the form of \n",
    "# the output from the processor built before\n",
    "tokens_batch = [[101, 2735, 102, 2737, 121, 4243, 1001],\n",
    "               [101, 219, 102, 2482, 1239, 1234, 102],\n",
    "               [101, 419, 102, 4202, 102, 0, 0]]\n",
    "tokens_batch = torch.tensor(tokens_batch)\n",
    "\n",
    "segs_batch = [[0, 0] + [1]*5,\n",
    "               [0, 0] + [1]*5,\n",
    "               [0, 0, 1, 1, 1, 0, 0]]\n",
    "segs_batch = torch.tensor(segs_batch)\n",
    "\n",
    "seg_ids = segment(tokens_batch); \n",
    "assert torch.equal(seg_ids, segs_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as we have a function that can retrieve the token_type_ids of our inputs on the fly, we can use it in the forward pass of our model.\n",
    "\n",
    "attention_mask is a one liner as we will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CustomTransformerModel(nn.Module):\n",
    "    \"\"\"custom transformer model for fast.ai\"\"\"\n",
    "    def __init__(self, transformer_model: PreTrainedModel):\n",
    "        super(CustomTransformerModel,self).__init__()\n",
    "        self.transformer = transformer_model\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        \n",
    "        attention_mask = (input_ids!=0).type(input_ids.type())\n",
    "        segmentation_mask = segment(input_ids).type(input_ids.type())\n",
    "        \n",
    "        logits = self.transformer(input_ids,\n",
    "                                  attention_mask=attention_mask,\n",
    "                                  token_type_ids=segmentation_mask)[0]\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the CustomTransformerModel \n",
    "config = config_class.from_pretrained(pretrained_model_name)\n",
    "config.num_labels = 4\n",
    "\n",
    "transformer_model = model_class.from_pretrained(pretrained_model_name, config = config)\n",
    "custom_transformer_model = CustomTransformerModel(transformer_model = transformer_model)\n",
    "\n",
    "custom_transformer_model.eval()\n",
    "try:\n",
    "    logits_batch = custom_transformer_model.forward(tokens_batch)\n",
    "except e:\n",
    "    print(e)\n",
    "\n",
    "assert logits_batch.shape == torch.Size([tokens_batch.shape[0], 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the other solutions we are customizing the numericalizer as well. Here we are lucky because there is no special needs to the different input form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TransformersVocab(Vocab):\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizer):\n",
    "        super(TransformersVocab, self).__init__(itos = [])\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def numericalize(self, t:Collection[str]) -> List[int]:\n",
    "        \"Convert a list of tokens `t` to their ids.\"\n",
    "        return self.tokenizer.convert_tokens_to_ids(t)\n",
    "        #return self.tokenizer.encode(t)\n",
    "\n",
    "    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n",
    "        \"Convert a list of `nums` to their tokens.\"\n",
    "        nums = np.array(nums).tolist()\n",
    "        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The customization work is done here. If you want to see an example how to use it, go to the 01_task3 notebook. There we applied it to the SemEval-2019 Task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_fastbert.ipynb.\n",
      "Converted 01_task3.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "consult",
   "language": "python",
   "name": "consult"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
