---

title: FastBert

keywords: fastai
sidebar: home_sidebar

summary: "fast.ai API customization for separateable sequence Bert model"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 00_fastbert.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
    
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># for the other models from huggingfaces go to</span>
<span class="c1"># https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta</span>
<span class="c1"># the notebook will work as well</span>
<span class="c1"># just pay attention at the customized model section</span>

<span class="n">model_class</span><span class="p">,</span> <span class="n">tokenizer_class</span><span class="p">,</span> <span class="n">config_class</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="p">,</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertConfig</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Bert-+-fast.ai">Bert + fast.ai<a class="anchor-link" href="#Bert-+-fast.ai">&#182;</a></h2><p>Customization is following the work of <a href="https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta">https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta</a>. I would recommemend to read the pipeline before, if not done already. When additional work is done we are explaining the steps.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First we are going into the implementations from former collueges and customize them to make the tokenizer capable for our special needs.</p>
<p>Namely we have to tokenize a List[sequence] element, where sequence in string form, to return a List[tokens] respecting the seperated format.</p>
<p>The respecting format is:</p>
\begin{equation*}
[CLS] + tokens(seq_1) + tokens(seq_2) + [SEP] + tokens(seq_3) + [SEP]
\end{equation*}<p>what will be cut by the max_len parameter of Bert.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TransformersBaseTokenizer" class="doc_header"><code>class</code> <code>TransformersBaseTokenizer</code><a href="https://github.com/PhilippMaxx/semeval2019_task3/tree/master/semeval2019_task3/fastbert.py#L19" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TransformersBaseTokenizer</code>(<strong><code>pretrained_tokenizer</code></strong>:<code>PreTrainedTokenizer</code>, <strong><code>model_type</code></strong>=<em><code>'bert'</code></em>, <strong><code>max_len</code></strong>=<em><code>64</code></em>, <strong>**<code>kwargs</code></strong>) :: <code>BaseTokenizer</code></p>
</blockquote>
<p>Wrapper around PreTrainedTokenizer to be compatible with fast.ai</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the Tokenizer class we just change the type annotations from str to List[str].</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SeqTokenizer" class="doc_header"><code>class</code> <code>SeqTokenizer</code><a href="https://github.com/PhilippMaxx/semeval2019_task3/tree/master/semeval2019_task3/fastbert.py#L38" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SeqTokenizer</code>(<strong><code>tok_func</code></strong>:<code>Callable</code>=<em><code>'SpacyTokenizer'</code></em>, <strong><code>lang</code></strong>:<code>str</code>=<em><code>'en'</code></em>, <strong><code>pre_rules</code></strong>:<code>Collection</code>[<code>Callable</code>[<code>str</code>, <code>str</code>]]=<em><code>None</code></em>, <strong><code>post_rules</code></strong>:<code>Collection</code>[<code>Callable</code>[<code>str</code>, <code>str</code>]]=<em><code>None</code></em>, <strong><code>special_cases</code></strong>:<code>Collection</code>[<code>str</code>]=<em><code>None</code></em>, <strong><code>n_cpus</code></strong>:<code>int</code>=<em><code>None</code></em>) :: <code>Tokenizer</code></p>
</blockquote>
<p>Put together rules and a tokenizer function to tokenize text with multiprocessing.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now the challenge is to make the TokenizeProcessor class able to use the new input form of List[sequence].</p>
<p>In fast.ai the TokenizeProcessor class processes a list of str to a concatination and uses then the tokenizer for the whole text. So we built a customized version for the Tokenizer. Here we changed the class functions to use our customized tokenizer and got rid of the function for concatination (_join_texts).</p>
<p>It would be more elegant to use the _join_texts function in a customized form to do our approach by concatinating the sentences via a special token. One problem with that approach would be that the BertTokenizer is not able to distinguish between text and special tokens in the text. We have to add the special tokens after tokenization.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="SeqTokenizeProcessor" class="doc_header"><code>class</code> <code>SeqTokenizeProcessor</code><a href="https://github.com/PhilippMaxx/semeval2019_task3/tree/master/semeval2019_task3/fastbert.py#L68" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>SeqTokenizeProcessor</code>(<strong><code>ds</code></strong>:<code>ItemList</code>=<em><code>None</code></em>, <strong><code>tokenizer</code></strong>:<code>Tokenizer</code>=<em><code>None</code></em>, <strong><code>chunksize</code></strong>:<code>int</code>=<em><code>10000</code></em>, <strong><code>mark_fields</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>include_bos</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>include_eos</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <code>TokenizeProcessor</code></p>
</blockquote>
<p><code>PreProcessor</code> that tokenizes the texts in <code>ds</code>.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># setting it up </span>
<span class="c1"># pretrained models can be shown by</span>
<span class="c1"># model_class.pretrained_model_archive_map.keys()</span>
<span class="n">pretrained_model_name</span> <span class="o">=</span> <span class="s1">&#39;bert-large-uncased-whole-word-masking&#39;</span>

<span class="n">transformer_tokenizer</span> <span class="o">=</span> <span class="n">tokenizer_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_model_name</span><span class="p">)</span>
<span class="n">transformer_base_tokenizer</span> <span class="o">=</span> <span class="n">TransformersBaseTokenizer</span><span class="p">(</span><span class="n">pretrained_tokenizer</span> <span class="o">=</span> <span class="n">transformer_tokenizer</span><span class="p">,</span> <span class="n">model_type</span> <span class="o">=</span> <span class="n">model_type</span><span class="p">)</span>
<span class="n">fastai_tokenizer</span> <span class="o">=</span> <span class="n">SeqTokenizer</span><span class="p">(</span><span class="n">tok_func</span> <span class="o">=</span> <span class="n">transformer_base_tokenizer</span><span class="p">,</span> <span class="n">pre_rules</span><span class="o">=</span><span class="p">[],</span> <span class="n">post_rules</span><span class="o">=</span><span class="p">[])</span>
<span class="n">tokenize_processor</span> <span class="o">=</span> <span class="n">SeqTokenizeProcessor</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">fastai_tokenizer</span><span class="p">,</span> <span class="n">include_bos</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">include_eos</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># testing of the TokenizeProcessor subclass</span>
<span class="n">test_item_1</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;turn one&#39;</span><span class="p">,</span><span class="s1">&#39;turn two&#39;</span><span class="p">,</span><span class="s1">&#39;turn three.&#39;</span><span class="p">]</span>
<span class="n">test_item_2</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;turn four&#39;</span><span class="p">,</span> <span class="s1">&#39;turn five&#39;</span><span class="p">,</span> <span class="s1">&#39;turn six.&#39;</span><span class="p">]</span>
<span class="n">test_tokens_1</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;[CLS]&#39;</span><span class="p">,</span> <span class="s1">&#39;turn&#39;</span><span class="p">,</span> <span class="s1">&#39;one&#39;</span><span class="p">,</span> <span class="s1">&#39;turn&#39;</span><span class="p">,</span> <span class="s1">&#39;two&#39;</span><span class="p">,</span> <span class="s1">&#39;[SEP]&#39;</span><span class="p">,</span> 
               <span class="s1">&#39;turn&#39;</span><span class="p">,</span> <span class="s1">&#39;three&#39;</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="s1">&#39;[SEP]&#39;</span><span class="p">]</span>
<span class="n">test_tokens_2</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;[CLS]&#39;</span><span class="p">,</span> <span class="s1">&#39;turn&#39;</span><span class="p">,</span> <span class="s1">&#39;four&#39;</span><span class="p">,</span> <span class="s1">&#39;turn&#39;</span><span class="p">,</span> <span class="s1">&#39;five&#39;</span><span class="p">,</span> <span class="s1">&#39;[SEP]&#39;</span><span class="p">,</span> 
               <span class="s1">&#39;turn&#39;</span><span class="p">,</span> <span class="s1">&#39;six&#39;</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="s1">&#39;[SEP]&#39;</span><span class="p">]</span>

<span class="n">test_items</span> <span class="o">=</span> <span class="n">ItemList</span><span class="p">(</span><span class="n">items</span> <span class="o">=</span> <span class="p">[</span><span class="n">test_item_1</span><span class="p">,</span> <span class="n">test_item_2</span><span class="p">])</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">tokenize_processor</span><span class="o">.</span><span class="n">process</span><span class="p">(</span><span class="n">test_items</span><span class="p">)</span>
<span class="k">except</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> 
    
<span class="k">assert</span> <span class="n">tokenize_processor</span><span class="o">.</span><span class="n">process_one</span><span class="p">([</span><span class="n">test_item_1</span><span class="p">])</span> <span class="o">==</span> <span class="n">test_tokens_1</span> 
<span class="k">assert</span> <span class="n">test_items</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">test_tokens_1</span>
<span class="k">assert</span> <span class="n">test_items</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">test_tokens_2</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that the tokenizer works like we wanted it to be, we have to move to the next problem. We need not only the input_ids of the sentences but also the attention_mask and token_type_ids.</p>
<p>When looking into the model setup we can do a simple trick by using utility functions in the forward pass. Hence we have two utitliy functions retrieving the masks from an input_ids batch.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="segment" class="doc_header"><code>segment</code><a href="https://github.com/PhilippMaxx/semeval2019_task3/tree/master/semeval2019_task3/fastbert.py#L85" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>segment</code>(<strong><code>input_ids</code></strong>)</p>
</blockquote>
<p>util function for token_type_ids in bert</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># testing the segment function</span>
<span class="c1"># 101 - CLS token, 102 - SEP token</span>
<span class="c1"># example batch is in the form of </span>
<span class="c1"># the output from the processor built before</span>
<span class="n">tokens_batch</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">2735</span><span class="p">,</span> <span class="mi">102</span><span class="p">,</span> <span class="mi">2737</span><span class="p">,</span> <span class="mi">121</span><span class="p">,</span> <span class="mi">4243</span><span class="p">,</span> <span class="mi">1001</span><span class="p">],</span>
               <span class="p">[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">219</span><span class="p">,</span> <span class="mi">102</span><span class="p">,</span> <span class="mi">2482</span><span class="p">,</span> <span class="mi">1239</span><span class="p">,</span> <span class="mi">1234</span><span class="p">,</span> <span class="mi">102</span><span class="p">],</span>
               <span class="p">[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">419</span><span class="p">,</span> <span class="mi">102</span><span class="p">,</span> <span class="mi">4202</span><span class="p">,</span> <span class="mi">102</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
<span class="n">tokens_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokens_batch</span><span class="p">)</span>

<span class="n">segs_batch</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">5</span><span class="p">,</span>
               <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">5</span><span class="p">,</span>
               <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
<span class="n">segs_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">segs_batch</span><span class="p">)</span>

<span class="n">seg_ids</span> <span class="o">=</span> <span class="n">segment</span><span class="p">(</span><span class="n">tokens_batch</span><span class="p">);</span> 
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">seg_ids</span><span class="p">,</span> <span class="n">segs_batch</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now as we have a function that can retrieve the token_type_ids of our inputs on the fly, we can use it in the forward pass of our model.</p>
<p>attention_mask is a one liner as we will see.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="CustomTransformerModel" class="doc_header"><code>class</code> <code>CustomTransformerModel</code><a href="https://github.com/PhilippMaxx/semeval2019_task3/tree/master/semeval2019_task3/fastbert.py#L107" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>CustomTransformerModel</code>(<strong><code>transformer_model</code></strong>:<code>PreTrainedModel</code>) :: <code>Module</code></p>
</blockquote>
<p>custom transformer model for fast.ai</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># testing the CustomTransformerModel </span>
<span class="n">config</span> <span class="o">=</span> <span class="n">config_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_model_name</span><span class="p">)</span>
<span class="n">config</span><span class="o">.</span><span class="n">num_labels</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">transformer_model</span> <span class="o">=</span> <span class="n">model_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">pretrained_model_name</span><span class="p">,</span> <span class="n">config</span> <span class="o">=</span> <span class="n">config</span><span class="p">)</span>
<span class="n">custom_transformer_model</span> <span class="o">=</span> <span class="n">CustomTransformerModel</span><span class="p">(</span><span class="n">transformer_model</span> <span class="o">=</span> <span class="n">transformer_model</span><span class="p">)</span>

<span class="n">custom_transformer_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">logits_batch</span> <span class="o">=</span> <span class="n">custom_transformer_model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">tokens_batch</span><span class="p">)</span>
<span class="k">except</span> <span class="n">e</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">logits_batch</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="n">tokens_batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">4</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As shown in the other solutions we are customizing the numericalizer as well. Here we are lucky because there is no special needs to the different input form.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TransformersVocab" class="doc_header"><code>class</code> <code>TransformersVocab</code><a href="https://github.com/PhilippMaxx/semeval2019_task3/tree/master/semeval2019_task3/fastbert.py#L125" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TransformersVocab</code>(<strong><code>tokenizer</code></strong>:<code>PreTrainedTokenizer</code>) :: <code>Vocab</code></p>
</blockquote>
<p>Contain the correspondence between numbers and tokens and numericalize.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The customization work is done here. If you want to see an example how to use it, go to the 01_task3 notebook. There we applied it to the SemEval-2019 Task 3.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">nbdev.export</span> <span class="kn">import</span> <span class="o">*</span>
<span class="n">notebook2script</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Converted 00_fastbert.ipynb.
Converted 01_task3.ipynb.
Converted index.ipynb.
</pre>
</div>
</div>

</div>
</div>

</div>
</div>
 

